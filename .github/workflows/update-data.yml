name: Update Match Data

on:
  schedule:
    - cron: '0 */3 * * *'       # Match data: every 3 hours
    - cron: '30 2 * * 1'        # Team discovery: Mondays at 02:30 UTC
  workflow_dispatch:
    inputs:
      refresh_teams:
        description: 'Re-discover teams from leagues'
        required: false
        default: 'false'
        type: choice
        options:
          - 'false'
          - 'true'
  push:
    branches: [main]
    paths:
      - 'teams.json'
      - 'leagues.json'
      - 'src/**'
      - 'generate_data.py'
      - 'scrape_teams.py'

permissions:
  contents: write

concurrency:
  group: "data-update"
  cancel-in-progress: false

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Restore data cache
        uses: actions/cache@v4
        with:
          path: cache
          key: data-cache-${{ github.run_id }}
          restore-keys: |
            data-cache-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install --no-cache-dir .

      # Team discovery runs weekly (Monday 02:30 UTC), on push, or when
      # manually triggered with refresh_teams=true. The 3-hourly match
      # scrape reuses the existing teams.json.
      - name: Discover teams from leagues
        if: |
          github.event.inputs.refresh_teams == 'true' ||
          github.event.schedule == '30 2 * * 1' ||
          github.event_name == 'push'
        run: python scrape_teams.py

      - name: Generate match data
        env:
          PUSHOVER_USER_KEY: ${{ secrets.PUSHOVER_USER_KEY }}
          PUSHOVER_API_TOKEN: ${{ secrets.PUSHOVER_API_TOKEN }}
        run: python generate_data.py

      - name: Copy leagues.json to public
        run: cp leagues.json public/data/leagues.json

      - name: Commit and push data
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add --force teams.json public/data/
          git diff --cached --quiet && echo "No data changes" && exit 0
          git commit -m "Update match data [skip ci]"
          git push

  notify-failure:
    runs-on: ubuntu-latest
    needs: [scrape]
    if: failure()
    steps:
      - name: Send Pushover failure notification
        env:
          PUSHOVER_USER_KEY: ${{ secrets.PUSHOVER_USER_KEY }}
          PUSHOVER_API_TOKEN: ${{ secrets.PUSHOVER_API_TOKEN }}
        run: |
          if [ -n "$PUSHOVER_USER_KEY" ] && [ -n "$PUSHOVER_API_TOKEN" ]; then
            curl -s \
              --form-string "token=$PUSHOVER_API_TOKEN" \
              --form-string "user=$PUSHOVER_USER_KEY" \
              --form-string "title=Esports Calendar: Data Update Failed" \
              --form-string "message=The data scraping workflow failed. Check: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}" \
              --form-string "priority=0" \
              https://api.pushover.net/1/messages.json
          fi
